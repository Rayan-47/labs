{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88b06a6e",
   "metadata": {},
   "source": [
    "ARTI308 - Machine Learning\n",
    "\n",
    "# Lab 4: Data Quality Assessment & Preprocessing\n",
    "\n",
    "In real-world machine learning projects, data is often:\n",
    "- Incomplete (missing values)\n",
    "- Noisy (outliers or random errors)\n",
    "- Inconsistent (wrong formats, mixed units)\n",
    "\n",
    "Before building any machine learning model, we must clean and prepare the data properly.\n",
    "\n",
    "In this lab, we will apply practical preprocessing techniques step by step on the `vgsales` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1333a",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df = pd.read_csv(\"vgsales.csv\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da158bd",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "### 2.1 Check Data Types\n",
    "Data types must match the real meaning of each column.\n",
    "For example:\n",
    "- `Year` should be integer-like year values\n",
    "- Sales columns should be numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e9fad",
   "metadata": {},
   "source": [
    "We observe that `Year` is stored as float because there are missing values.\n",
    "`Platform`, `Genre`, and `Publisher` can be stored as categorical features for better memory efficiency.\n",
    "Correct data types support proper numerical analysis and preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20b69e",
   "metadata": {},
   "source": [
    "### 2.2 Convert Incorrect Data Types\n",
    "We will convert:\n",
    "- `Year` to nullable integer (`Int64`)\n",
    "- `Platform`, `Genre`, and `Publisher` to categorical type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in ['Platform', 'Genre', 'Publisher']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138baa3",
   "metadata": {},
   "source": [
    "Now `Year` is stored as nullable integer (`Int64`).\n",
    "Categorical columns are properly encoded as `category`.\n",
    "This improves type consistency and memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3f33b",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values\n",
    "### 3.1 Detect Missing Values\n",
    "Missing values reduce data quality and can affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19aece3",
   "metadata": {},
   "source": [
    "The output shows whether any column contains missing values.\n",
    "In `vgsales`, missing values are commonly found in `Year` and `Publisher`.\n",
    "These missing values should be handled before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4031e",
   "metadata": {},
   "source": [
    "### 3.2 Demonstration: Introduce Artificial Missing Values\n",
    "### Why?\n",
    "\n",
    "Although the dataset already has missing values in some columns, we introduce artificial missing values in a numeric feature for imputation practice.\n",
    "\n",
    "we will be running this line:\n",
    "\n",
    "`df_missing.loc[0:5, 'Global_Sales'] = np.nan`\n",
    "\n",
    "\n",
    "- `df_missing`: The pandas DataFrame you are modifying.\n",
    "\n",
    "- `.loc[0:5, 'Global_Sales']`: This uses the label-based indexer to select specific rows and columns.\n",
    "\n",
    "- `0:5`: Selects rows with index labels 0, 1, 2, 3, 4, and 5. In label-based indexing, the end index is inclusive.\n",
    "\n",
    "- `'Global_Sales'`: Selects the column named 'Global_Sales'.\n",
    "\n",
    "- `= np.nan`: Assigns the value np.nan (which stands for \"Not a Number\") to all the selected cells. This is the standard way to represent missing or null values in numerical columns in pandas. The column's data type remains numeric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = df.copy()\n",
    "df_missing.loc[0:5, 'Global_Sales'] = np.nan\n",
    "df_missing.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9e943",
   "metadata": {},
   "source": [
    "Now the `Global_Sales` column contains additional missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape: \",df.shape)\n",
    "print(\"After removing some values: \",df_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78391f",
   "metadata": {},
   "source": [
    "### Strategy 1: Remove Records\n",
    "This strategy removes records containing missing data.\n",
    "It works well if the number of missing rows is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09661fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed = df_missing.dropna()\n",
    "df_removed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d544b73",
   "metadata": {},
   "source": [
    "The dataset now has fewer rows.\n",
    "If only a small portion of data was missing, this method is acceptable.\n",
    "\n",
    "However, removing too many rows can reduce model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc43dc1",
   "metadata": {},
   "source": [
    "### Strategy 2: Mean Imputation\n",
    "\n",
    "![Mean.png](Mean.png)\n",
    "\n",
    "The mean represents the average value.\n",
    "It is commonly used for normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741150f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_mean = df_missing.copy()\n",
    "df_imputed_mean['Global_Sales'] = df_imputed_mean['Global_Sales'].fillna(df_imputed_mean['Global_Sales'].mean())\n",
    "\n",
    "df_imputed_mean.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_mean.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a9ad",
   "metadata": {},
   "source": [
    "Missing values are now replaced with the average global sales.\n",
    "This preserves dataset size but may reduce variability.\n",
    "Mean imputation is sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b8da5",
   "metadata": {},
   "source": [
    "### Strategy 3: Median Imputation\n",
    "\n",
    "![median_formula_2.png](median_formula_2.png)\n",
    "The median is more robust to outliers than the mean.\n",
    "It is preferred for skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c24f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_median = df_missing.copy()\n",
    "df_imputed_median['Global_Sales'] = df_imputed_median['Global_Sales'].fillna(df_imputed_median['Global_Sales'].median())\n",
    "\n",
    "df_imputed_median.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_median.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64625cca",
   "metadata": {},
   "source": [
    "Missing values are replaced with the middle value.\n",
    "This approach is safer when data contains extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e0b57",
   "metadata": {},
   "source": [
    "## 4. Handling Outliers\n",
    "Outliers are extreme values that can distort models.\n",
    "We will detect outliers in `Global_Sales` using the IQR method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=df['Global_Sales'])\n",
    "plt.title(\"Boxplot of Global_Sales\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1bebf",
   "metadata": {},
   "source": [
    "Points outside the whiskers represent potential outliers.\n",
    "These extreme sales values may influence model predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911b78e",
   "metadata": {},
   "source": [
    "### Detect Outliers using IQR\n",
    "**Method: Interquartile Range (IQR)**\n",
    "\n",
    "The IQR method defines outliers as values outside:\n",
    "\n",
    "`Q1 - 1.5×IQR`  and  `Q3 + 1.5×IQR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['Global_Sales'].quantile(0.25)\n",
    "Q3 = df['Global_Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['Global_Sales'] < lower) | (df['Global_Sales'] > upper)]\n",
    "outliers.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42402af7",
   "metadata": {},
   "source": [
    "The output displays records considered extreme based on statistical boundaries.\n",
    "These may be valid high-value transactions or potential data errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2594ada",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "We remove values outside the acceptable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df[(df['Global_Sales'] >= lower) & (df['Global_Sales'] <= upper)]\n",
    "print(\"Original shape: \",df.shape)\n",
    "print(\"After removing outliers: \",df_no_outliers.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25939e",
   "metadata": {},
   "source": [
    "The dataset size is slightly reduced.\n",
    "Removing outliers reduces distortion but may also remove important rare events.\n",
    "\n",
    "#### Important Note on Removing Outliers\n",
    "\n",
    "Not all outliers are errors.\n",
    "\n",
    "Some extreme values may represent rare but important real-world events.  \n",
    "For example, in a sales dataset, a very large transaction might correspond to a bulk corporate order or a seasonal promotion.  \n",
    "\n",
    "If we remove such values blindly, we may lose valuable information and bias the analysis.\n",
    "\n",
    "Before removing outliers, we should always ask:\n",
    "- Is this value a data entry mistake?\n",
    "- Or is it a valid but rare observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8593d3",
   "metadata": {},
   "source": [
    "### Capping Outliers (Percentile Method)\n",
    "Instead of removing outliers, we replace extreme values with percentile limits.\n",
    "\n",
    "![percentile.png](percentile.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_cap = df['Global_Sales'].quantile(0.05)\n",
    "upper_cap = df['Global_Sales'].quantile(0.95)\n",
    "\n",
    "df_capped = df.copy()\n",
    "df_capped['Global_Sales'] = df_capped['Global_Sales'].clip(lower_cap, upper_cap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807230d1",
   "metadata": {},
   "source": [
    "## 5. Data Transformation ? Normalization\n",
    "Normalization scales numerical features to a similar range.\n",
    "This ensures that no feature influences the model simply because it has larger numerical values.\n",
    "\n",
    "### Min-Max Normalization\n",
    "Min-Max normalization rescales numerical values to a fixed range, usually between 0 and 1.\n",
    "\n",
    "It works using the formula:\n",
    "\n",
    "This method preserves the original distribution shape and relative ordering of values.\n",
    "\n",
    "Min-Max normalization is especially useful for distance-based models such as:\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- K-Means clustering\n",
    "- Support Vector Machines (SVM)\n",
    "\n",
    "These models rely on distance calculations, and if features are on very different scales, one feature can dominate the distance computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['NA_Sales', 'EU_Sales']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ecb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df[['NA_Sales', 'EU_Sales']].copy()\n",
    "\n",
    "df_scaled[['NA_Sales', 'EU_Sales']] = scaler.fit_transform(df_scaled)\n",
    "\n",
    "df_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0abf3a",
   "metadata": {},
   "source": [
    "After applying Min-Max normalization, all numerical values are scaled to the range between 0 and 1.\n",
    "\n",
    "The smallest value in each feature becomes 0, and the largest becomes 1.\n",
    "All other values are proportionally mapped between these two limits.\n",
    "\n",
    "Importantly, normalization does NOT change the relative relationships between data points.\n",
    "If one game originally had higher regional sales than another, it will still have a higher normalized value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de94f15",
   "metadata": {},
   "source": [
    "### Z-Score Normalization\n",
    "Z-score standardization transforms the data so that:\n",
    "\n",
    "- The mean of each feature becomes 0\n",
    "- The standard deviation becomes 1\n",
    "\n",
    "This is done by subtracting the mean and dividing by the standard deviation:\n",
    "\n",
    "![zscore.png](zscore.png)\n",
    "\n",
    "This method keeps the shape of the distribution but rescales it around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd95ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_standardized = df[['NA_Sales', 'EU_Sales']].copy()\n",
    "\n",
    "df_standardized[['NA_Sales', 'EU_Sales']] = scaler.fit_transform(df_standardized)\n",
    "\n",
    "df_standardized.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e1fed",
   "metadata": {},
   "source": [
    "After standardization, the numerical features are centered around 0.\n",
    "Values above the original mean become **positive**, and values below the mean become **negative**.\n",
    "\n",
    "The standard deviation of each feature becomes approximately 1, meaning the spread of the data is standardized.\n",
    "\n",
    "This transformation is especially useful for:\n",
    "- Linear regression\n",
    "- Support Vector Machines (SVM)\n",
    "- PCA\n",
    "\n",
    "Because these models assume features are centered and scaled similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8624d9",
   "metadata": {},
   "source": [
    "## Check Correlation Before Applying PCA\n",
    "\n",
    "we will check whether numerical features are correlated. If features are strongly correlated, they contain overlapping information.\n",
    "\n",
    "- **Correlation close to 1**  → Strong positive linear relationship  \n",
    "  (As one feature increases, the other also increases.)\n",
    "\n",
    "- **Correlation close to -1** → Strong negative linear relationship  \n",
    "  (As one feature increases, the other decreases.)\n",
    "\n",
    "- **Correlation close to 0**  → Weak or no linear relationship  \n",
    "  (The features do not move together in a predictable linear way.)\n",
    "\n",
    "In such cases, dimensionality reduction using PCA is meaningful \n",
    "because we can combine correlated features into fewer components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb490cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(df_standardized[['NA_Sales','EU_Sales']].corr(), \n",
    "            annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap (Before PCA)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb57aa",
   "metadata": {},
   "source": [
    "The heatmap shows the correlation between the numerical features:\n",
    "\n",
    "- The diagonal values are 1 because each feature is perfectly correlated with itself.\n",
    "- The correlation between `NA_Sales` and `EU_Sales` is approximately *0.768*.\n",
    "\n",
    "A positive value close to 1 indicates a **strong linear** relationship between the two features.\n",
    "\n",
    "This means that `NA_Sales` and `EU_Sales` often increase together across many games.\n",
    "\n",
    "Since PCA is most useful when features are correlated,\n",
    "applying PCA here can meaningfully summarize the shared information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c48412e",
   "metadata": {},
   "source": [
    "## 6. Data Reduction – Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique.\n",
    "\n",
    "Instead of working directly with the original features, PCA creates new features called **principal components**.\n",
    "\n",
    "These components:\n",
    "\n",
    "- Are linear combinations of the original features\n",
    "- Are uncorrelated with each other\n",
    "- Capture variance in descending order (from most important to least)\n",
    "\n",
    "The first principal component (PC1) captures the largest possible variance in the dataset.\n",
    "\n",
    "The second principal component (PC2) captures the next largest variance, while being orthogonal (perpendicular) to PC1.\n",
    "\n",
    "This allows us to reduce dimensionality while retaining most of the important information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43d7dd",
   "metadata": {},
   "source": [
    "### Visual Intuition\n",
    "\n",
    "Imagine we have two features:\n",
    "\n",
    "X1 = NA_Sales  \n",
    "X2 = EU_Sales  \n",
    "\n",
    "If we plot the data points, they may look like this:\n",
    "\n",
    "              X2\n",
    "               |\n",
    "               |\n",
    "               |        *\n",
    "               |      *\n",
    "               |    *\n",
    "               |  *\n",
    "               | *\n",
    "               |________________________ X1\n",
    "\n",
    "Notice that the points follow a diagonal pattern.\n",
    "This means the two features are correlated and contain overlapping information.\n",
    "\n",
    "Instead of keeping both X1 and X2 separately,\n",
    "PCA finds the direction where the data varies the most.\n",
    "\n",
    "That direction becomes **Principal Component 1 (PC1)**.\n",
    "\n",
    "              X2\n",
    "               |\n",
    "               |        *\n",
    "               |      *\n",
    "               |    *\n",
    "               |  *\n",
    "               | *\n",
    "               |________________________ X1\n",
    "                    \\\n",
    "                     \\\n",
    "                      \\\n",
    "                       \\\n",
    "                        -> PC1 (maximum variance direction)\n",
    "\n",
    "PC2 is the direction perpendicular to PC1.\n",
    "\n",
    "If most of the variation is along PC1,\n",
    "then PC1 alone captures most of the dataset?s information.\n",
    "\n",
    "In that case, we can reduce:\n",
    "\n",
    "2 features -> 1 feature (PC1)\n",
    "\n",
    "while keeping most of the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_standardized[['NA_Sales', 'EU_Sales']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fb912",
   "metadata": {},
   "source": [
    "The `Explained Variance Ratio` indicates how much of the total information (variance) is captured by each principal component.\n",
    "\n",
    "For example:\n",
    "- If PC1 explains 85% of the variance, it means that one new feature already summarizes most of the dataset's information.\n",
    "- If PC1 and PC2 together explain nearly 100%, then very little information is lost.\n",
    "\n",
    "When most of the variance is captured by fewer components, dimensionality reduction is considered effective.\n",
    "\n",
    "This helps simplify models, reduce computational cost, and sometimes improve generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0009d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # In addition to the heatmap, we can visualize the relationship between two features using a scatter plot. \n",
    "# # A scatter plot allows us to observe whether the features exhibit a linear pattern, meaning whether they increase or decrease together in a consistent way.\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.scatter(df_standardized['NA_Sales'], df_standardized['EU_Sales'])\n",
    "# plt.xlabel(\"NA_Sales (Standardized)\")\n",
    "# plt.ylabel(\"EU_Sales (Standardized)\")\n",
    "# plt.title(\"Scatter Plot: NA_Sales vs EU_Sales\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(principal_components[:,0], principal_components[:,1])\n",
    "plt.title(\"PCA Projection\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67208c6",
   "metadata": {},
   "source": [
    "Each point in this plot represents one game record.\n",
    "\n",
    "The axes no longer represent the original features (NA_Sales and EU_Sales).\n",
    "Instead:\n",
    "\n",
    "- The horizontal axis represents Principal Component 1 (PC1).\n",
    "- The vertical axis represents Principal Component 2 (PC2).\n",
    "\n",
    "PC1 captures the direction of maximum variance in the data.\n",
    "PC2 captures the second most important direction, perpendicular to PC1.\n",
    "\n",
    "The spread of points along the horizontal direction indicates how much variation is captured by PC1.\n",
    "If most of the spread appears horizontally, it suggests that PC1 captures most of the dataset's information.\n",
    "\n",
    "This projection allows us to visualize high-dimensional data in a lower-dimensional space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605f240",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment, you will:\n",
    "- **Task 1**\n",
    "Identify data quality issues in the `vgsales` dataset.\n",
    "\n",
    "- **Task 2**\n",
    "Apply one missing value strategy and explain why.\n",
    "\n",
    "- **Task 3**\n",
    "Detect and handle outliers using IQR.\n",
    "\n",
    "- **Task 4**\n",
    "Normalize numerical features using both Min-Max and Z-score.\n",
    "\n",
    "- **Task 5**\n",
    "Apply PCA and interpret explained variance.\n",
    "\n",
    "\n",
    "End of lab 4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}